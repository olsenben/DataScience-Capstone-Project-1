{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Classifier Training\n",
    "To perform classification with a number of difference algorithms, extract text features for Bag of Words analysis, use those features to train a classifier, then evaluate its performance on a test set. In this notebook I will classify the cleaned `mbti_cleaned_unsplit1.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>enfp intj moments sportscenter top ten plays p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im finding lack posts alarming sex boring posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one course say know blessing curse absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear intp enjoyed conversation day esoteric ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre fired another silly misconception approa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                         clean_posts  \n",
       "0  enfp intj moments sportscenter top ten plays p...  \n",
       "1  im finding lack posts alarming sex boring posi...  \n",
       "2  good one course say know blessing curse absolu...  \n",
       "3  dear intp enjoyed conversation day esoteric ga...  \n",
       "4  youre fired another silly misconception approa...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data\n",
    "df = pd.read_csv('mbti_cleaned_unsplit2.csv', encoding = \"'ISO-8859-1\")\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "#select only entries with no null values\n",
    "df = df[pd.notnull(df['clean_posts'])]\n",
    "df = df[pd.notnull(df['posts'])]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Bag of Words\n",
    "The steps to prepare text for classification (post cleaning) is to create a matrix of token counts of the words (bag of words). After that, the counts are normalized using `TfidfTransformer`. Make sure to save both `CountVectorizer` and `TfidfTransformer` in global variables, because when the time comes to extract features from the test set, we will need access to each in order to make sure the test data uses the same amount of features as the training data, otherwise the trained model won't be compatible with the features of the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#define the text and labels to be used as corpus, labels\n",
    "corpus = np.array(df.clean_posts)\n",
    "labels = np.array(df.type)\n",
    "#declare count_vect and tfidf_transformer\n",
    "count_vect = CountVectorizer(binary=False, ngram_range=(1,1))\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created funtions to perform each step of feature extraction, tf-idf transformation, and classifier testing so as to be able to debug any problems step by step. This entire process can be performed by creating a training pipeline utilizing ```Pipeline``` from ```sklearn.pipeline```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perpare_data_set(corpus, labels, test_size=0.3):\n",
    "    \"splits data into training corpus, test corpus, training labels, test labels\"\n",
    "    train_c, test_c, train_l, test_l  = train_test_split(corpus, labels, test_size=test_size, random_state=42)\n",
    "    return train_c, test_c, train_l, test_l\n",
    "\n",
    "def cv_feature_extract(train_c):\n",
    "    \"tokenizes text data\"\n",
    "    return count_vect.fit_transform(train_c)\n",
    "\n",
    "def fit_transform(X_train_counts):\n",
    "    \"re-weights tokenized data\"\n",
    "    return tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "def test_model(test_corpus, test_labels, fit_model):\n",
    "    \"tests the predictions of a classifier against test data\"\n",
    "    extracted = count_vect.transform(test_corpus)\n",
    "    transformed = tfidf_transformer.transform(extracted)\n",
    "    predicted = fit_model.predict(transformed)\n",
    "    return np.mean(predicted == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the above functions to the data. Pause after ```fit_transform``` to check how many features have been extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6071, 116161)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data into train and test\n",
    "train_corpus, test_corpus, train_labels, test_labels = perpare_data_set(corpus, labels, test_size=0.3)\n",
    "\n",
    "#extract features\n",
    "X_train_counts = cv_feature_extract(train_corpus)\n",
    "X_train_tfidf = fit_transform(X_train_counts)\n",
    "\n",
    "#examine features\n",
    "X_train_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice 115959 features have been extracted from 6071 data entries. For clarity's sake, check the shape of the test data to make sure it has the same number of features when extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2603, 116161)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine features\n",
    "cv_test_features = count_vect.transform(test_corpus)\n",
    "cv_test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. If they do not match, it is because they used diffirent cases of CountVectorizer() and TfidfTransformer(), and will not be compatible when it comes time to test the classifier's predicitions.\n",
    "## Classifier Training\n",
    "The following is the training of classifiers using the algorithms `MultinomialNB`, `LogisticRegression`, and `SGDClassifier`.\n",
    "### `MultinomialNB`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#train classifier mnb\n",
    "mnb = MultinomialNB().fit(X_train_counts, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40261237034191316"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test classifier against test data.\n",
    "test_model(test_corpus, test_labels, mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultinomialNB` was only successful at prediciting the label of test data 40%. \n",
    "### `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#train classifier lr\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
    "lr.fit(X_train_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63349980791394545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test classifier against test data.\n",
    "test_model(test_corpus, test_labels, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` was successful at predicition the label of test data 63% of the time. Quite an improvement.\n",
    "### `SGDClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#train classifier sgdc\n",
    "sgdc = SGDClassifier()\n",
    "sgdc.fit(X_train_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67345370726085285"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test classifier against test data.\n",
    "test_model(test_corpus, test_labels, sgdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `SGDClassifier` was successful at predicition the label of test data 67% of the time. Unsurprisingly, `SGDClassifier` is widely considered to be one of the more useful algorithms for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning and Cross Validation\n",
    "Having tested a few different classification methods, it's time to select the most sucessful one and perform cross validation using `GridSearchCV` to select the best parameters. This step can be computationally expensive and time consuming, so test parameters incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1'], 'l1_ratio': [0.15, 0.3, 0.5], 'max_iter': [10, 100, 200], 'loss': ['hinge', 'log'], 'n_jobs': [-1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify a parameter grid to search over\n",
    "parameters_2 = {\n",
    "    'penalty': ['l1'],\n",
    "    'l1_ratio': [0.15, 0.3, 0.5],\n",
    "    'max_iter': [10, 100, 200],\n",
    "    'loss': ['hinge', 'log'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "sgdc_cv2 = GridSearchCV(sgdc, parameters_2, cv=5) #specify GridSearchCV object\n",
    "\n",
    "sgdc_cv2.fit(X_train_tfidf, train_labels) #fit to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check and see what parameters GridSearchCV selected. This is useful if you want to go back and test other parameters without rechecking certain parameters unnecessarily. Here we can see that it selected `l1_ratio=0.3`, `max_iter=10`, and `penalty=l1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1_ratio': 0.5,\n",
       " 'loss': 'hinge',\n",
       " 'max_iter': 10,\n",
       " 'n_jobs': -1,\n",
       " 'penalty': 'l1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdc_cv2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the new cross-validated model on new test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67883211678832112"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_extracted = count_vect.transform(test_corpus)\n",
    "X_test_transformed = tfidf_transformer.transform(X_test_extracted)\n",
    "\n",
    "sgdc_cv2.score(X_test_transformed, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a 0.6% improvement in accuracy is nothing to sneeze at, especially if this model was taken to scale.\n",
    "## Model Evaluation\n",
    "Below I've borrowed a few metrics for evaluating the model performance in more detail from Dipanjan Sarkar's repository for his book Text Analytics with Python, viewable @ https://github.com/dipanjanS/text-analytics-with-python, which includes a useful homegrown library for model performance evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
    "    'takes predicted classifications and compares them to true labels in a matrix'\n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
    "                                  labels=classes)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
    "                                                  labels=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
    "                                                labels=level_labels)) \n",
    "    return (cm_frame) # I've adjusted this function to return a dataframe rather than print it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"16\" halign=\"left\">Predicted:</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">Actual:</th>\n",
       "      <th>ENFJ</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>58</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>476</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>228</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>311</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Predicted:                                                    \\\n",
       "                   ENFJ ENFP ENTJ ENTP ESFJ ESFP ESTJ ESTP INFJ INFP INTJ   \n",
       "Actual: ENFJ         13    3    0    0    0    0    1    0    9   13    4   \n",
       "        ENFP          1  125    0    9    0    0    0    1   17   28   16   \n",
       "        ENTJ          1    6   31    6    1    0    1    0    4    4    5   \n",
       "        ENTP          0   15    0  119    0    0    3    0   13    7   17   \n",
       "        ESFJ          0    0    0    2    1    0    0    0    1    1    0   \n",
       "        ESFP          0    1    0    0    0    0    0    0    3    2    1   \n",
       "        ESTJ          0    3    1    1    0    0    0    0    3    4    0   \n",
       "        ESTP          0    0    0    1    0    0    0    6    6    1    7   \n",
       "        INFJ          1    9    1   18    0    0    1    0  304   58   19   \n",
       "        INFP          1   15    0    9    0    0    8    1   24  476   20   \n",
       "        INTJ          0    7    5    6    0    0    3    0   25   19  228   \n",
       "        INTP          0    1    0    9    0    2    4    1   17   36   17   \n",
       "        ISFJ          0    0    0    1    0    0    0    0    8    6    2   \n",
       "        ISFP          0    1    0    0    0    1    2    0   11   15    5   \n",
       "        ISTJ          0    5    0    2    0    0    1    0    2    8   14   \n",
       "        ISTP          0    2    0    6    0    0    0    0    3    3    6   \n",
       "\n",
       "                                       \n",
       "             INTP ISFJ ISFP ISTJ ISTP  \n",
       "Actual: ENFJ    8    0    1    1    2  \n",
       "        ENFP    7    0    2    0    0  \n",
       "        ENTJ    5    0    2    0    2  \n",
       "        ENTP   15    0    2    1    1  \n",
       "        ESFJ    3    0    1    1    0  \n",
       "        ESFP    0    0    1    0    1  \n",
       "        ESTJ    1    0    1    1    0  \n",
       "        ESTP    2    0    0    0    2  \n",
       "        INFJ   10    0    2    1    4  \n",
       "        INFP   17    0    8    2    5  \n",
       "        INTJ   20    2    1    3    1  \n",
       "        INTP  311    2    0    0    3  \n",
       "        ISFJ    5   30    1    0    1  \n",
       "        ISFP    2    0   31    0    2  \n",
       "        ISTJ    5    1    2   25    0  \n",
       "        ISTP    6    1    2    0   67  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the trained model to predict processed test data\n",
    "y_pred = sgdc_cv2.predict(X_test_transformed)\n",
    "labels = list(np.unique(df.type)) #make a list of unique target variables\n",
    "\n",
    "#display confusion matrix\n",
    "cm_df = display_confusion_matrix(true_labels=test_labels, predicted_labels=y_pred, classes=labels)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       ENFJ       0.76      0.24      0.36        55\n",
      "       ENFP       0.65      0.61      0.63       206\n",
      "       ENTJ       0.82      0.46      0.58        68\n",
      "       ENTP       0.63      0.62      0.62       193\n",
      "       ESFJ       0.50      0.10      0.17        10\n",
      "       ESFP       0.00      0.00      0.00         9\n",
      "       ESTJ       0.00      0.00      0.00        15\n",
      "       ESTP       0.67      0.24      0.35        25\n",
      "       INFJ       0.68      0.71      0.69       428\n",
      "       INFP       0.70      0.81      0.75       586\n",
      "       INTJ       0.63      0.71      0.67       320\n",
      "       INTP       0.75      0.77      0.76       403\n",
      "       ISFJ       0.83      0.56      0.67        54\n",
      "       ISFP       0.54      0.44      0.49        70\n",
      "       ISTJ       0.71      0.38      0.50        65\n",
      "       ISTP       0.74      0.70      0.72        96\n",
      "\n",
      "avg / total       0.68      0.68      0.67      2603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print percision and recall scores for model\n",
    "print(metrics.classification_report(test_labels, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that representation bias seems to have a large effect on the recall of the model. Some categories couldn't be successfully classified because their representation was so low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Utilizing a Pipeline\n",
    "Here is some sample code for building a pipline for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
       "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#declare classifier text_clf\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of performing feature extraction then training classifier `text_clf` in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train classifier text_clf\n",
    "text_clf.fit(train_corpus, train_labels)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
